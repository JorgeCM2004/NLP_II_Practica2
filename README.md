# NLP II PrÃ¡ctica 2

# 1. InstalaciÃ³n

```bash

```

# 3. Probar la practica

Para probar los resultados simplemenete se deberÃ¡ ejecutar el notebook llamado `Notebook_P2.ipynb`.

## 3.Estructura del proyecto

```
NLP_II_Practica2/
â”œâ”€â”€ src/   
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py   
â”‚   â”œâ”€â”€ text_preprocessor.py   
â”‚   â”œâ”€â”€ embedding_extractor.py  
â”‚   â”œâ”€â”€ dense_retriever.py   
â”‚   â”œâ”€â”€ knn_classifier.py  
â”‚   â”œâ”€â”€ hybrid_classifier.py   
â”‚   â”œâ”€â”€ model_trainer.py   
â”‚   â”œâ”€â”€ compressor.py   
â”‚   â”œâ”€â”€ summarizer.py  
â”‚   â”œâ”€â”€ explainability.py   
â”‚   â””â”€â”€ evaluator.py  
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Notebook_P2.ipynb  
â”œâ”€â”€ models/   
â”œâ”€â”€ data/
â””â”€â”€ README.md
â””â”€â”€ pyproject.toml
```

## MÃ³dulos implementados

| MÃ³dulo                    | DescripciÃ³n                                  |
| -------------------------- | --------------------------------------------- |
| `DataLoader`             | Descarga y armoniza los datos                 |
| `TextPreprocessor`       | Tokenization and text cleaning                |
| `EmbeddingExtractor`     | Dense embeddings from Transformer encoder     |
| `DenseRetriever`         | k-NN index for similarity search              |
| `KNNClassifier`          | Classification via majority voting            |
| `HybridClassifier`       | Transformer + k-NN combination (Î± parameter) |
| `ModelTrainer`           | Training with logging and checkpoints         |
| `ModelCompressor`        | Knowledge distillation                        |
| `SummarizationExplainer` | T5/BART for generating explanations           |
| `ExplainabilityModule`   | Case-based reasoning + LLM explanations       |
| `Evaluator`              | Metrics, confusion matrices, plots            |

## ğŸ§ª Experiments

The notebook covers:

1. **Â§4.1 Dense Retrieval**: Build index, evaluate Precision@k, Recall@k
2. **Â§4.2 k-NN Classifier**: Majority voting, compare with baselines
3. **Â§4.3 Hybrid RAG**: Experiment with Î± values (0.0 â†’ 1.0)
4. **Â§4.4 Explainability**: Case-based reasoning for 20 examples
5. **Â§4.5 Compression**: DistilBERT vs RoBERTa (speed/quality)
6. **Â§4.6 Summarization**: Global class summaries, local explanations

## â±ï¸ Estimated Runtime

| Phase                               | Time (GPU)           |
| ----------------------------------- | -------------------- |
| Data loading & preprocessing        | ~2 min               |
| Embedding extraction                | ~10-15 min           |
| k-NN index building                 | ~1-2 min             |
| Distilled model training (3 epochs) | ~20-30 min           |
| Summarization                       | ~5-10 min            |
| **Total**                     | **~40-60 min** |

## ğŸ› ï¸ Models Used

| Role       | Model                         |
| ---------- | ----------------------------- |
| Teacher    | `roberta-base` (fine-tuned) |
| Student    | `distilbert-base-uncased`   |
| Summarizer | `t5-small`                  |

## ğŸ“Š Metrics

- Accuracy
- Macro F1
- Per-class F1
- Precision@k / Recall@k (retrieval)
- Inference time (ms/sample)
- Model size (MB)
